<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>hzw&#39; blog</title>
  <icon>https://www.gravatar.com/avatar/c316d07af70f73a6862590e79322d097</icon>
  <subtitle>今日事，今日毕</subtitle>
  <link href="http://hzwy3c.github.io.git/atom.xml" rel="self"/>
  
  <link href="http://hzwy3c.github.io.git/"/>
  <updated>2022-10-23T03:53:05.199Z</updated>
  <id>http://hzwy3c.github.io.git/</id>
  
  <author>
    <name>hzw</name>
    <email>3067608875@qq.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>解释器与编译器的区别</title>
    <link href="http://hzwy3c.github.io.git/2022/10/23/%E8%A7%A3%E9%87%8A%E5%99%A8%E5%92%8C%E7%BC%96%E8%AF%91%E5%99%A8%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>http://hzwy3c.github.io.git/2022/10/23/%E8%A7%A3%E9%87%8A%E5%99%A8%E5%92%8C%E7%BC%96%E8%AF%91%E5%99%A8%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2022-10-23T03:52:39.073Z</published>
    <updated>2022-10-23T03:53:05.199Z</updated>
    
    <content type="html"><![CDATA[<h2 id="解释器与编译器的区别"><a href="#解释器与编译器的区别" class="headerlink" title="解释器与编译器的区别"></a>解释器与编译器的区别</h2><p>两者都是将高级语言转换成机器码，解释器在程序运行时将代码转换成机器码，编译器在程序运行之前将代码转换成机器码。</p><p>（编译器相当于做好一桌子菜再开吃，解释器就是吃火锅边煮边吃，吃火锅效率要低一点）</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/%E8%A7%A3%E9%87%8A%E5%99%A8%E5%92%8C%E7%BC%96%E8%AF%91%E5%99%A8%E7%9A%84%E5%8C%BA%E5%88%AB.assets/image-20221023113917661.png" alt="image-20221023113917661"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;解释器与编译器的区别&quot;&gt;&lt;a href=&quot;#解释器与编译器的区别&quot; class=&quot;headerlink&quot; title=&quot;解释器与编译器的区别&quot;&gt;&lt;/a&gt;解释器与编译器的区别&lt;/h2&gt;&lt;p&gt;两者都是将高级语言转换成机器码，解释器在程序运行时将代码转换成机器码，编译器</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>ConGLR(Incorporating Context Graph with Logical Reasoning for Inductive Relation Prediction)</title>
    <link href="http://hzwy3c.github.io.git/2022/10/18/ConGLR/"/>
    <id>http://hzwy3c.github.io.git/2022/10/18/ConGLR/</id>
    <published>2022-10-18T09:38:56.893Z</published>
    <updated>2022-10-18T11:22:43.108Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018143509940.png" alt="image-20221018143509940"></p><p>今天分享一篇SIGIR 2022的论文，题目名为Incorporating Context Graph with Logical Reasoning for Inductive Relation Prediction，从题目可以看出这篇论文是做Inductive任务的，同时还建立了Context Graph来满足逻辑推理的需要。</p><p>论文地址：<a href="https://web.archive.org/web/20220711200404id_/https://dl.acm.org/doi/pdf/10.1145/3477495.3531996">https://web.archive.org/web/20220711200404id_/https://dl.acm.org/doi/pdf/10.1145/3477495.3531996</a></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>由于实体独立关系建模的需求和离散逻辑推理的解释性二者难以权衡，所以目前无法精确进行归纳关系预测。</p><p>本文提出了一种新的模型ConGLR，将context graph和逻辑推理结合起来。首先，提取并初始化目标头尾实体的封闭子图。然后介绍了包含关系路径、关系和实体的context graph。其次，利用两个具有实体和关系信息交互的图卷积网络分别处理子图和context graph。考虑到不同边缘和目标关系的影响，本文为子图GCN引入了边缘感知和关系感知注意机制。最后，通过将关系路径作为规则体，目标关系作为规则头，将神经计算和逻辑推理结合起来，得到归纳分数。</p><h2 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h2><p>这里声明了两个定义</p><ul><li><p>Connected and Closed Horn Rule. 就是形如下面的霍恩规则：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018151559153.png" alt="image-20221018151559153"></p></li></ul><p>这里$\epsilon$用来表示这个霍恩规则的置信度</p><ul><li>Relational Path. 就是关系路径，一些连续的关系可以表示为一条关系路径</li></ul><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018151926720.png" alt="image-20221018151926720"></p><p>之后本文将规则头对应为目标关系，规则体对应为关系路径，关系置信度对应为注意力权重。这也是文中如何将context graph与逻辑推理结合的。</p><p>下面是文中一些数学符号的含义：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018152129137.png" alt="image-20221018152129137"></p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p>ConGLR模型的整体架构图如下所示：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018160055103.png" alt="image-20221018160055103"></p><h4 id="Subgraph-and-Context-Graph-Preprocessing"><a href="#Subgraph-and-Context-Graph-Preprocessing" class="headerlink" title="Subgraph and Context Graph Preprocessing"></a>Subgraph and Context Graph Preprocessing</h4><p>子图的提取和初始化为GraIL中的方法一样，主要介绍一下context graph的提取。</p><p>context graph的提取基于子图。context graph的节点集合包括子图中的实体、关系和关系路径，context graph的关系集合包括关系上下文和路径上下文，关系上下文用来连接实体和关系，路径上下文用来连接关系路径和关系。文中用一个图很清楚地描述了context graph的提取过程：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018153001368.png" alt="image-20221018153001368"></p><h4 id="Context-Graph-Modeling"><a href="#Context-Graph-Modeling" class="headerlink" title="Context Graph Modeling"></a>Context Graph Modeling</h4><p>这部分包含节点表示初始化和特征聚合两个方面。</p><p>context gtaph节点分为三个部分，需要对这三个部分分别初始化。对于实体节点，本文直接使用子图中的double radius embedding对其初始化；对于关系节点，本文设置一个可学习的嵌入矩阵对其初始化；对于一个关系路径$p_i&#x3D;(r_1,r_2,\cdots,r_t)$，本文使用一个前馈神经网络对其初始化：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018153749693.png" alt="image-20221018153749693"></p><p>其中$o_{r_t}^0$表示嵌入矩阵中对关系$r_t$的表示。</p><p>本文使用下式进行特征聚合：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018153942524.png" alt="image-20221018153942524"></p><p>$h_u^k$是第k层节点u的嵌入向量，$c_w^k$是第k层上下文（边）w的嵌入。$\phi$函数定义如下：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018154230048.png" alt="image-20221018154230048"></p><p>文中说$\phi$用这个式子是因为它强大的表现能力。</p><h4 id="Subgraph-Modeling"><a href="#Subgraph-Modeling" class="headerlink" title="Subgraph Modeling"></a>Subgraph Modeling</h4><p>本文使用了一种具有边缘感知和关系感知注意机制的GCN来处理子图，它用下式来更新节点：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018154555538.png" alt="image-20221018154555538"></p><p>其中$(i,l,j)$是一个triple，$(i,l)$是j的邻居节点和邻居关系元组，$e_j^k$是实体j在第k层的嵌入，$o_l^k$是关系l在第k层的嵌入（在context graph中得到），$\alpha_{(i,l,j)}^k$是注意力权重，它由下式计算：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018155136013.png" alt="image-20221018155136013"></p><p>其中$\beta$是edge-aware attention（中心节点邻接边感知），$\gamma$是relation-aware attention（中心节点邻接关系的感知）。</p><p>刚才从context graph中拿来了关系嵌入，有来有回，这里也会使用实体嵌入来增强context graph中的实体表示：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018155628922.png" alt="image-20221018155628922"></p><p>本文还使用bidirectional Gated Recurrent Unit (GRU)来对实体嵌入处理，用于增强模型的表现能力。</p><h4 id="Inductive-Prediction-and-Training-Regime"><a href="#Inductive-Prediction-and-Training-Regime" class="headerlink" title="Inductive Prediction and Training Regime"></a>Inductive Prediction and Training Regime</h4><p>最后在子图第k层可以得到实体的嵌入<strong>E</strong>，在context graph第k层可以得到节点的嵌入<strong>H</strong>。规则的置信度$\mu$由关系路径与目标关系嵌入点乘计算：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018160617222.png" alt="image-20221018160617222"></p><p>目标关系路径的嵌入就是关系路径嵌入的加权和：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018161045774.png" alt="image-20221018161045774"></p><p>最后一个三元组$(h_T,r_T,t_T)$的分数为：<br><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018161113837.png" alt="image-20221018161113837"></p><p>Loss函数还是经典的Loss</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018161151929.png" alt="image-20221018161151929"></p><p>至此ConGLR模型就介绍完了，最后放上一个该模型的总体算法流程图：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018161241168.png" alt="image-20221018161241168"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;https://gitcode.net/qq_44866969/pic/-/raw/master/ConGLR.assets/image-20221018143509940.png&quot; alt=&quot;image-20221018143509940&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>一文读懂Embedding</title>
    <link href="http://hzwy3c.github.io.git/2022/08/28/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82Embedding/"/>
    <id>http://hzwy3c.github.io.git/2022/08/28/%E4%B8%80%E6%96%87%E8%AF%BB%E6%87%82Embedding/</id>
    <published>2022-08-28T12:21:51.469Z</published>
    <updated>2022-08-28T12:31:16.856Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、什么是Embedding？"><a href="#一、什么是Embedding？" class="headerlink" title="一、什么是Embedding？"></a>一、<strong>什么是Embedding？</strong></h2><p>“Embedding”直译是<strong>嵌入式、嵌入层</strong>。</p><p>简单来说，我们常见的<strong>地图</strong>就是对于<strong>现实地理的Embedding</strong>，现实的地理地形的信息其实远远超过三维，但是地图通过颜色和等高线等来最大化表现现实的地理<strong>信息</strong>。</p><p>通过它，我们在现实世界里的文字、图片、语言、视频就能转化为计算机能识别、能使用的语言，且转化的过程中信息不丢失。</p><p>Embedding层，在某种程度上，就是用来降维的，降维的原理就是矩阵乘法。</p><h2 id="二、One-Hot编码"><a href="#二、One-Hot编码" class="headerlink" title="二、One-Hot编码"></a>二、One-Hot编码</h2><p>One-Hot 编码是<strong>分类变量</strong>作为<strong>二进制向量</strong>的表示。</p><ol><li>将分类值映射到整数值。</li><li>然后，每个整数值被表示为二进制向量，除了整数的索引之外，它都是零值，它被标记为1。</li></ol><p><strong>举例：</strong></p><p>运动特征：[“足球”，”篮球”，”羽毛球”，”乒乓球”]（N&#x3D;4）：</p><p>足球 &#x3D;&gt; [1,0,0,0]</p><p>篮球 &#x3D;&gt; [0,1,0,0]</p><p>羽毛球 &#x3D;&gt; [0,0,1,0]</p><p>乒乓球 &#x3D;&gt; [0,0,0,1]</p><p><strong>优点：</strong></p><ol><li>解决了 <strong>分类器不好处理离散数据</strong> 的问题。将离散型特征使用 one-hot 编码，会让<strong>特征之间的距离计算</strong>更加合理。</li><li>在一定程度上也起到了 <strong>扩充特征</strong> 的作用。</li></ol><p><strong>缺点：</strong></p><p>在文本特征表示上有些缺点就非常突出了。</p><ol><li>它是一个词袋模型，不考虑<strong>词与词之间的顺序</strong>（文本中词的顺序信息也是很重要的）；</li><li>它<strong>假设词与词相互独立</strong>（在大多数情况下，词与词是相互影响的）；</li><li>它得到的<strong>特征是离散稀疏</strong>的 (这个问题最严重，Embedding降维就是对此优化)。</li></ol><h2 id="三、怎么理解Embedding"><a href="#三、怎么理解Embedding" class="headerlink" title="三、怎么理解Embedding"></a>三、<strong>怎么理解Embedding</strong></h2><p>由于One-Hot编码过于稀疏，过度占用资源，我们使用Embedding对其降维。</p><p>假设：我们有一个2 x 6的矩阵，然后乘上一个6 x 3的矩阵后，变成了一个2 x 3的矩阵。</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/Embedding.assets/image-20220828162714414.png" alt="image-20220828162714414"></p><p>Embedding层，在某种程度上，就是用来降维的，降维的原理就是<strong>矩阵乘法</strong>。</p><p>可以降维，那么embedding也可以升维，<strong>对低维的数据进行升维时，可能把一些其他特征给放大了，或者把笼统的特征给分开了</strong>。</p><h2 id="四、Word-Embedding"><a href="#四、Word-Embedding" class="headerlink" title="四、Word Embedding"></a><strong>四、Word Embedding</strong></h2><p>比如将词汇表里的词用 “Royalty”, “Masculinity”, “Femininity” 和 “Age” 4个维度来表示，King 这个词对应的词向量<strong>可能</strong>是 (0.99,0.99,0.05,0.7)。</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/Embedding.assets/image-20220828162731969.png" alt="image-20220828162731969"></p><p>在实际情况中，有时并不能<strong>对词向量的每个维度做一个很好的解释</strong>，但我们知道他是某一个维度的特征就可以了。</p><p>这个过程称为 <strong>word embedding（词嵌入）</strong>，即将高维词向量嵌入到一个低维空间。如图：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/Embedding.assets/image-20220828162742048.png" alt="image-20220828162742048"></p><p>经过我们一系列的降维神操作，有了用 representation 表示的较短的词向量，我们就可以较容易的分析词之间的关系了，比如我们将词的维度降维到 2维，有一个有趣的研究表明，用下图的词向量表示我们的词时，我们可以发现：</p><p><img src="https://gitcode.net/qq_44866969/pic/-/raw/master/Embedding.assets/image-20220828162752774.png" alt="image-20220828162752774"></p><p>queen（皇后）&#x3D; king（国王）- man（男人）+ woman（女人）</p><p>这样计算机能明白，“皇后啊，就是女性的国王”</p><p>walked（过去式）&#x3D; walking（进行时）- swimming（进行时）+ swam（过去式）</p><p>同理计算机也能明白，“walked，就是walking的过去式”</p><p>另外，向量间的距离也可能会建立联系，比方说“北京”是“中国”的首都，“巴黎”是“法国”的首都，那么向量：|中国|-|北京|&#x3D;|法国|-|巴黎|</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;一、什么是Embedding？&quot;&gt;&lt;a href=&quot;#一、什么是Embedding？&quot; class=&quot;headerlink&quot; title=&quot;一、什么是Embedding？&quot;&gt;&lt;/a&gt;一、&lt;strong&gt;什么是Embedding？&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;“</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>myFirstBlog</title>
    <link href="http://hzwy3c.github.io.git/2022/05/05/myFirstBlog-markdown/"/>
    <id>http://hzwy3c.github.io.git/2022/05/05/myFirstBlog-markdown/</id>
    <published>2022-05-05T13:00:04.000Z</published>
    <updated>2022-05-05T13:02:25.824Z</updated>
    
    
    
    
    
    <category term="firstblog" scheme="http://hzwy3c.github.io.git/tags/firstblog/"/>
    
    <category term="hexo" scheme="http://hzwy3c.github.io.git/tags/hexo/"/>
    
    <category term="markdown" scheme="http://hzwy3c.github.io.git/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://hzwy3c.github.io.git/2022/05/03/hello-world/"/>
    <id>http://hzwy3c.github.io.git/2022/05/03/hello-world/</id>
    <published>2022-05-03T13:09:50.002Z</published>
    <updated>2022-05-03T13:18:38.740Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Hello-Hexo"><a href="#Hello-Hexo" class="headerlink" title="Hello Hexo"></a>Hello Hexo</h2><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
